{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed16e6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ce25dfa89c4baca715d3f1e9602c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import login\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "login()\n",
    "\n",
    "checkpoint = \"openai/clip-vit-large-patch14\"\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-image-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbe809b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89414dbf806a40e3baeeca64ea3ad98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawings = load_dataset(\"imagefolder\", data_dir=\"Drawings/\")\n",
    "ds = drawings['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8913479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['image'] = ds['image'].apply(lambda x: x.get('path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification\n",
    "# Load evaluator\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(evals):\n",
    "    predictions, labels = evals\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61137428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for index, row in ds.iterrows():\n",
    "    image = Image.open(row['image'])\n",
    "    pred = detector(image, candidate_labels=[0,1])\n",
    "    prediction = pred[0]['label']\n",
    "    preds.append(prediction)\n",
    "    labels.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7a17b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4738372093023256}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(predictions=preds, references=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picturethat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
